{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6567501d-07ec-4e35-bf9c-99132240aab3",
   "metadata": {},
   "source": [
    "# Vector Calculus\n",
    "\n",
    "## Differentiation\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} := \\lim \\limits_{h \\to \\infty} \\frac{f(x+h)-f(x)}{h}\n",
    "$$\n",
    "\n",
    "**Taylor Series:** The Taylor series is a function with an infinite sum of terms that are expressed in derivatives at a single point x<sub>0</sub> for a function <i>f</i>.\n",
    "\n",
    "$$\n",
    "T_n(x) := \\sum \\limits _{k=0} ^n \\frac{f^{(k)}(x_0)}{k!} (x-x_0)^k\n",
    "$$\n",
    "\n",
    "**Rules**:\n",
    "\n",
    "Product rule:\n",
    "\n",
    "$$\n",
    "(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)\n",
    "$$\n",
    "\n",
    "Quotient rule:\n",
    "\n",
    "$$\n",
    "\\bigg(\\frac {f(x)}{g(x)}\\bigg)' = \\frac {f'(x)g(x) = f(x)g'(x)}{(g(x))^2}\n",
    "$$\n",
    "\n",
    "Sum rule:\n",
    "\n",
    "$$\n",
    "(f(x)+g(x))' = f'(x)+g'(x)\n",
    "$$\n",
    "\n",
    "Chain rule:\n",
    "\n",
    "$$\n",
    "(g(f(x)))' = (g \\circ f)'(x) = g'(f(x))f'(x)\n",
    "$$\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "To require the gradients w.r.t. the parameter set *&theta;*, the partial dervatives of *L* w.r.t. to the parameters ***&theta;**<sub>j</sub>* = { ***A**<sub>j</sub>*, ***b**<sub>j</sub>* } of each layer j = 0, ..., K-1. The chain rule allows the partial derivatives as following:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial \\theta_{K-1}} = \\frac {\\partial L}{\\partial f_K} \\frac {\\partial f_K}{\\partial \\theta_{K-1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial L}{\\partial \\theta_i} = \\frac {\\partial L}{\\partial f_K} \\frac {\\partial f_K}{\\partial f_{K-1}} \\cdots \\frac {\\partial f_{i+2}}{\\partial f_{i+1}} \\frac {\\partial f_{i+1}}{\\partial \\theta_i}\n",
    "$$\n",
    "\n",
    "## Automatic Differentiation\n",
    "\n",
    "Via the chain rule and a computation graph the output y is differentiated w.r.t. to the input parameters. The chain rule is in general associative. Applying the chain rule backwards is referred to as reverse mode whereas the the opposite is called forward mode. In the context of Deep Learning, the dimensionality of the input is often much higher than the dimnesionality of the labels. Therefore the reverse mode is computationally significantly cheaper than the forward mode.\n",
    "\n",
    "Reverse mode:\n",
    "$$\n",
    "\\frac {dy}{dx} = \\bigg(\\frac {dy}{db} \\frac {db}{da}\\bigg) \\frac {da}{dx}\n",
    "$$\n",
    "\n",
    "Forward mode:\n",
    "$$\n",
    "\\frac {dy}{dx} = \\frac {dy}{db} \\bigg(\\frac {db}{da} \\frac {da}{dx}\\bigg)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9271f09-4cb2-460c-b17f-acd3789f7e85",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "- [Deisenroth., M., Faisal, A., Ong, C. S. (2020). Mathematics for Machine Learning. Cambridge.](https://mml-book.github.io/book/mml-book.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
