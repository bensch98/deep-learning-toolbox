{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f579a2-38da-4123-8dd0-4459d173d2e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning Math\n",
    "\n",
    "## Notation\n",
    "\n",
    "**Scalar:** Just a single value.\n",
    "\n",
    "**Vector:** Array of numbers, which are arranged in order.\n",
    "\n",
    "**Matrix:** 2-D array of numbers, which is why each element is indexed by two subscripts.\n",
    "\n",
    "From left to right: Column Vector, Row Vector, Matrix\n",
    "$$\n",
    "\\begin{align}\n",
    "    x &= \\begin{bmatrix}\n",
    "       x_{1} \\\\\n",
    "       x_{2} \\\\\n",
    "       \\vdots \\\\\n",
    "       x_{m}\n",
    "     \\end{bmatrix}, \\ \\ \\ \\ x^T = \\begin{bmatrix}\n",
    "       x_{1} & x_{2} & \\cdots & x_{n}\n",
    "     \\end{bmatrix}, \\ \\ \\ \\ X = \\begin{bmatrix}\n",
    "       x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\\n",
    "       x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n",
    "       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\n",
    "     \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "One special matrix is the identity matrix. The identity matrix does not change any vector when it is multplied by that matrix. It is defined as:\n",
    "$$ A^{-1}A=I_n\n",
    "\\begin{align}\n",
    "    &= \\begin{bmatrix}\n",
    "       1 & 0 & 0 \\\\\n",
    "       0 & 1 & 0 \\\\\n",
    "       0 & 0  & 1\n",
    "     \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "**Tensor:** More generalized form of a matrix with arbitrary many axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087ce24-160b-4ff7-a0fc-133baa7cdf79",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Operations\n",
    "\n",
    "**Transpose:** Mirror image of the matrix across a diagonal line from top left to bottom right.\n",
    "$$ (A^T)_{i,j} = A_{j,i} $$\n",
    "**Addition:** The addition of two matrices is defined as the elementwise addition of their elements. In order to sum two matrices, they both have to be of the same size. In Computer Science the concept of broadcasting is often applied to match the sizes automatically in case only one of the dimensions is correct and the other one is 1. Scalar can also be applied via broadcasting.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    A + B := \\begin{bmatrix}\n",
    "       a_{1,1} + b_{1,1} & \\cdots & a_{1,n} + b_{1,n} \\\\\n",
    "       \\vdots  & \\ddots & \\vdots \\\\\n",
    "       a_{m,1} + b_{m,1} & \\cdots & a_{m,n} + b_{m,n}\n",
    "     \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Multiplication:** There are different kinds of multiplication on matrices. Matrix multiplication by default is *not* defined as the elementwise product. The elementwise so called *Hadamard product* is sometimes used in Computer Science, especially in combination with broadcasting to apply certain operation to all columns or all rows even if the dimensions don't match. The default matrix multiplication is defined as:\n",
    "$$\n",
    "{\\textit C_{i,j}} = \\sum \\limits _k {\\textit A_{i,k} B_{k,j}}\n",
    "$$\n",
    "**Example:**\n",
    "$$\n",
    "\\begin{align}\n",
    "    For \\ A &= \\begin{bmatrix}\n",
    "       1 & 2 & 3 \\\\\n",
    "       3 & 2 & 1\n",
    "     \\end{bmatrix} \\in \\mathbb{R}^{2x3}, \\\n",
    "     \\begin{bmatrix}\n",
    "       0 & 2 \\\\\n",
    "       1 & -1 \\\\\n",
    "       0 & 1\n",
    "     \\end{bmatrix} \\in \\mathbb{R}^{3x2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    AB &= \\begin{bmatrix}\n",
    "       1 & 2 & 3 \\\\\n",
    "       3 & 2 & 1\n",
    "     \\end{bmatrix}\n",
    "     \\begin{bmatrix}\n",
    "       0 & 2 \\\\\n",
    "       1 & -1 \\\\\n",
    "       0 & 1\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "       2 & 3 \\\\\n",
    "       2 & 5\n",
    "     \\end{bmatrix} \\in \\mathbb{R}^{2,2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    BA &= \\begin{bmatrix}\n",
    "       0 & 2 \\\\\n",
    "       1 & -1 \\\\\n",
    "       0 & 1\n",
    "     \\end{bmatrix}\n",
    "     \\begin{bmatrix}\n",
    "       1 & 2 & 3 \\\\\n",
    "       3 & 2 & 1\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "       6 & 4 & 2 \\\\\n",
    "       -2 & 0 & 2 \\\\\n",
    "       3 & 2 & 1\n",
    "     \\end{bmatrix} \\in \\mathbb{R}^{3,3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This example shows that matrix multiplication is not commutative. Furthermore the so called dot product from this example is only defined if the neighboring dimensions match.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa38ef5-ee46-45cd-90db-52fd46778e2f",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "In order to measure the size of a vector, in machine learning, usually the norm is used for that. Norms are functions mapping vectors to non-negative values. Intuitively the norm of a vector x measures the distance from the origin to point x. The most important norm used is the L<sup>2</sup> norm, which is also known as the Euclidian norm for measuring the distance between the origin and point x and is defined for p = 2. More generally a L<sup>p</sup> norm is defined as:\n",
    "$$\n",
    "\\| \\mathbf{x} \\|_p = \\bigg( \\sum \\limits _i | x_i |^p \\bigg)^{\\frac{1}{p}}\n",
    "$$\n",
    "\n",
    "To measure the size of a whole matrix the *Frobenius norm* is the most common to do this. The *Frobenius norm* can be simplified with the Trace() function, which sums up all diagonal entries of a matrix:\n",
    "$$\n",
    "Tr(A) = \\sum \\limits _i A_{i,i} \\ \\ , \\ \\ \\|A\\|_F = \\sqrt{Tr(AA^T)} \\ \\ , \\ \\ \\|A\\|_F = \\sqrt{ \\sum \\limits{i,j} A_{i,j}^{2} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4873f150-87a0-4fd5-85a1-23188018236c",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "- [Goodfellow., I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.](https://www.deeplearningbook.org/contents/linear_algebra.html)\n",
    "- [Deisenroth., M., Faisal, A., Ong, C. S. (2020). Mathematics for Machine Learning. Cambridge.](https://mml-book.github.io/book/mml-book.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
